{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing GAN generated data augmentation, using different proportions of data for training GAN \n",
    "## This was \n",
    "\n",
    "Dataset is COVID_QU_Ex dataset from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "#This code needs a little bit rework, so testing will be easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './CovidData/Lung_Segmentation_Data/'\n",
    "\n",
    "CLASSES = ['normal', 'viral', 'covid']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.Resize(size=(128, 128)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.Resize(size=(128, 128)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_directories = {\n",
    "    'Test_orig_0.8' : '2022-12-14_12-57-44',\n",
    "    'Test_orig_0.6' : '2022-12-14_15-04-49',\n",
    "    'Test_orig_0.4' : '2022-12-14_16-48-34',\n",
    "    'Test_orig_0.2' : '2022-12-14_18-54-13',\n",
    "    \n",
    "    'Test_0_0.8' : '2022-12-15_09-10-08',\n",
    "    'Test_0_0.6' : '2022-12-15_10-41-13',\n",
    "    'Test_0_0.4' : '2022-12-15_11-34-52',\n",
    "    'Test_0_0.2' : '2022-12-15_12-31-45',\n",
    "\n",
    "    'Test_1_0.8' : '2022-12-15_12-57-15',\n",
    "    'Test_1_0.6' : '2022-12-15_14-07-49',\n",
    "    'Test_1_0.4' : '2022-12-15_15-01-39',\n",
    "    'Test_1_0.2' : '2022-12-15_15-41-37',\n",
    "\n",
    "    'Test_2_0.8' : '2022-12-15_16-08-24',\n",
    "    'Test_2_0.6' : '2022-12-15_17-22-46',\n",
    "    'Test_2_0.4' : '2022-12-15_18-18-53',\n",
    "    'Test_2_0.2' : '2022-12-15_19-04-53',\n",
    "\n",
    "    'Test_3_0.8' : '2022-12-15_19-31-00',\n",
    "    'Test_3_0.6' : '2022-12-15_20-43-34',\n",
    "    'Test_3_0.4' : '2022-12-15_21-39-35',\n",
    "    'Test_3_0.2' : '2022-12-15_22-21-06',\n",
    "     }\n",
    "#TODO Make this into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset): #should be ImageFolder, needs rewrite\n",
    "    def __init__(self, images, classes, transform):\n",
    "        #self.class_names = classes\n",
    "        #self.image_dirs = image_dirs\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.images))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, idx = self.images[index]\n",
    "        with open(path, 'r') as file:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), idx\n",
    "\n",
    "def DatasetMaker(split, mode=None, data_ratio=1, transform = None, geoaugment=False, seed = 0):\n",
    "    \"\"\"\n",
    "        Returns a CustomDataset with given parameters\n",
    "        split: str, which train-test to use; options: 'orig', '0', '1', '2', '3'\n",
    "        mode: str, 'oversampling', 'gan'\n",
    "            'oversampling' : oversample with real images, to balance classes\n",
    "                    'gan' : balance datasets with gan generated images (uses data_ratio to figure out which gan to use)\n",
    "                    None  : dataset won't be balanced\n",
    "        data_ratio (optional): int, the ratio of the training covid data to be used, valid_ratios:\n",
    "                        '1' : all data\n",
    "                        '0.8' : 80% of training images\n",
    "                        '0.6' : 60% of training images\n",
    "                        '0.4' : 40% of training images\n",
    "                        '0.2' : 20% of training images\n",
    "        transform (optional): torch.Compose instance, sets the dataset's transforms\n",
    "        geougment (optional): bool, uses basic data augmentation means\n",
    "        seed (optional): int, seed to use for reproducibility\n",
    "    \"\"\"\n",
    "    #when test is orig, and gan is gan_1, should this be used? what? \n",
    "    if is_test_valid(split) is False:\n",
    "        return\n",
    "    \n",
    "    classes = ['normal', 'viral', 'covid']\n",
    "    orig_dirs = {\n",
    "    'normal' : f'{root_dir}/original/Normal',\n",
    "    'viral' : f'{root_dir}/original/Non-Covid',\n",
    "    'covid' : f'{root_dir}/original/COVID-19'\n",
    "    }\n",
    "    fake_dirs = {\n",
    "        'gan_0.8' : f'{root_dir}/generated/Test_{split}/gan_0.8',\n",
    "        'gan_0.6' : f'{root_dir}/generated/Test_{split}/gan_0.6',\n",
    "        'gan_0.4' : f'{root_dir}/generated/Test_{split}/gan_0.4',\n",
    "        'gan_0.2' : f'{root_dir}/generated/Test_{split}/gan_0.2'\n",
    "    }\n",
    "    indicies_files = {\n",
    "        'gan_1' : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_1_gan.pkl',\n",
    "        'gan_0.8' : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_0.8_gan.pkl',\n",
    "        'gan_0.6' : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_0.6_gan.pkl',\n",
    "        'gan_0.4' : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_0.4_gan.pkl',\n",
    "        'gan_0.2' : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_0.2_gan.pkl',\n",
    "        'test'  : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_test.pkl',\n",
    "        'train'  : f'{root_dir}/Indicies_files/Test_{split}/{split}_split_train_and_val.pkl'\n",
    "    }\n",
    "    class_idx = {\n",
    "        'covid': 0, \n",
    "        'viral': 1,\n",
    "        'normal': 2,\n",
    "        'gan_1' : 0,\n",
    "        'gan_0.8' : 0,\n",
    "        'gan_0.6' : 0,\n",
    "        'gan_0.4' : 0,\n",
    "        'gan_0.2' : 0\n",
    "    }\n",
    "    idx_to_class ={\n",
    "        0: 'covid',\n",
    "        1: 'viral',\n",
    "        2: 'normal'\n",
    "    }\n",
    "    valid_ratios = [1, 0.8, 0.6, 0.4, 0.2]\n",
    "\n",
    "    source_dir = {}\n",
    "    train_images = []\n",
    "\n",
    "    #We make the source dir for the classes\n",
    "    for class_name in classes: \n",
    "        source_dir[class_name] = orig_dirs[class_name]\n",
    "    \n",
    "    file = indicies_files['train'] #Get all training images\n",
    "    imgs = load_images_from_file(file)\n",
    "    imgs = [*imgs[0],*imgs[1]] #Bad file generation\n",
    "    num_of_covid_imgs = 0\n",
    "    for x in imgs: #Set the training images path \n",
    "        class_of_x = idx_to_class[x[1]]\n",
    "        item = os.path.join(source_dir[class_of_x],x[0]),class_idx[class_of_x] \n",
    "        if item[1] == class_idx['covid']: num_of_covid_imgs +=1 \n",
    "        train_images.append(item)\n",
    "    if data_ratio!=1: #If data_ratio is not 1, we need to change the covid images of the dataset\n",
    "        num_of_covid_imgs = 0\n",
    "        train_images = [x for x in train_images if x[1]!=class_idx['covid']]\n",
    "        file = indicies_files[f'gan_{data_ratio}']\n",
    "        imgs = load_images_from_file(file)\n",
    "        for x in imgs:\n",
    "            class_of_x = idx_to_class[x[1]]\n",
    "            item = os.path.join(source_dir[class_of_x],x[0]),class_idx[class_of_x] \n",
    "            if item[1] == class_idx['covid']: num_of_covid_imgs +=1 \n",
    "            train_images.append(item)\n",
    "\n",
    "    average_class_size = round((len(train_images)-num_of_covid_imgs)/2)\n",
    "    missing_images = max(0, average_class_size - num_of_covid_imgs)\n",
    "\n",
    "    if mode=='gan' and (data_ratio in valid_ratios):\n",
    "        #If gan is used for dataset balancing\n",
    "        gan = f'gan_{data_ratio}'\n",
    "        gan_dir = fake_dirs[gan]\n",
    "        generate_images_to_dir(split, data_ratio, gan, gan_dir, missing_images) #?\n",
    "        gan_ims = []\n",
    "        for x in os.listdir(gan_dir): #optimize further\n",
    "            if x.lower().endswith('jpg'):\n",
    "                item = os.path.join(gan_dir, x), class_idx['covid']\n",
    "                gan_ims.append(item)\n",
    "        sample = random.sample(gan_ims, missing_images)\n",
    "        train_images = [*train_images,*sample]\n",
    "    elif mode=='oversampling':\n",
    "        covid_images = [x for x in train_images if x[1]==class_idx['covid']]\n",
    "        batch_size = len(covid_images)\n",
    "        while batch_size <= missing_images:\n",
    "            train_images = [*train_images, *covid_images]\n",
    "            missing_images -= batch_size\n",
    "        if missing_images>0:\n",
    "            sample = random.sample(covid_images, missing_images)\n",
    "            missing_images = [*missing_images, *sample]\n",
    "    \n",
    "    test_images = []\n",
    "    test_imgs = load_images_from_file(indicies_files['test'])\n",
    "    for x in test_imgs:\n",
    "        class_of_x = idx_to_class[x[1]]\n",
    "        item = os.path.join(source_dir[class_of_x],x[0]),idx_to_class[class_of_x] #This should be correct\n",
    "        test_images.append(item)\n",
    "\n",
    "    if transform is None:\n",
    "        transforms = [ torchvision.transforms.Resize(size=(128, 128)),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.Grayscale(num_output_channels=1)]\n",
    "    else:\n",
    "        transforms = transform\n",
    "    if geoaugment:\n",
    "        augmentation_transforms = [#torchvision.transforms.RandomHorizontalFlip(), #(should be useful, causes confusion with gans)\n",
    "                                torchvision.transforms.RandomAffine(4)]\n",
    "        transforms = [augmentation_transforms + transforms]                      \n",
    "    transforms = torchvision.transforms.Compose(transforms)\n",
    "    train_dataset = CustomDataset(train_images, classes, transforms)\n",
    "    test_dataset = CustomDataset(test_images, classes, transforms)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def load_images_from_file(file):\n",
    "    with open(file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data   \n",
    "    \n",
    "def generate_images_to_dir(split, data_ratio, gan, dir, size):\n",
    "    \"\"\"\n",
    "        Generates pictures with a given gan, to a given directory\n",
    "    \"\"\"\n",
    "    curr_dir = os.getcwd()\n",
    "    output_dir = os.path.join(curr_dir, dir) #?\n",
    "    gan_dir = gan_directories[f'Test_{split}_{data_ratio}'] \n",
    "\n",
    "    lippi_dir = '/home/bbernard/lipizzaner-covidgan-master/src' #Change this on server\n",
    "\n",
    "    code =f'conda activate lipizzaner && python main.py generate --mixture-source ./output/lipizzaner_gan/master/{gan_dir} -o {output_dir} --sample-size {size} -f configuration/covid-qu-conv/Test_{split}/covidqu_{data_ratio}.yml'\n",
    "    os.system.chdir(lippi_dir)\n",
    "    subprocess.run(code)\n",
    "    os.system.chdir(curr_dir)\n",
    "\n",
    "\n",
    "def is_test_valid(test):\n",
    "    if test in ['orig', '0', '1', '2', '3']: return True\n",
    "    else: return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(name):\n",
    "    if name==\"resnet\":\n",
    "        resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "        resnet18.conv1= torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        resnet18.fc = torch.nn.Linear(in_features=512, out_features=3)\n",
    "        resnet18.get_name = 'resnet18'\n",
    "        return resnet18\n",
    "    elif name==\"vgg\":\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        vgg16.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        vgg16.classifier[6] = torch.nn.Linear(in_features=4096, out_features=3, bias=True)\n",
    "        vgg16.get_name = 'vgg16'\n",
    "        return vgg16\n",
    "    elif name==\"efficient\":\n",
    "        efficient = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "        efficient.features[0][0] = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        efficient.classifier[1] = torch.nn.Linear(in_features=1280, out_features=3, bias=True)\n",
    "        efficient.get_name = 'efficientnet_b0'\n",
    "        return efficient\n",
    "    else:\n",
    "        print(\"Not implemented\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27132\n"
     ]
    }
   ],
   "source": [
    "#params = {'split': 'orig',      #'orig' and '0' through '3'\n",
    "#            'mode'      : None, #'oversampling', 'gan'\n",
    "#            'data_ratio': 1,    # 1, 0.8, 0.6, 0.4, 0.2\n",
    "#            'transform' : None, \n",
    "#            'geoaugment': False,\n",
    "#            'seed'      : 0\n",
    "#            }\n",
    "\n",
    "#train_dataset, test_dataset = DatasetMaker(params['split'], params['mode'], params['data_ratio']) #, params['transform'], params['geoaugment'], params['seed']) \n",
    "\n",
    "#BATCH=64\n",
    "#train_dl = DataLoader(train_dataset, batch_size= BATCH, shuffle = True)\n",
    "#test_dl = DataLoader(test_dataset, batch_size= BATCH, shuffle = True )\n",
    "\n",
    "#print(f'Number of train images: {len(train_dataset)}, number of test images: {len(test_dataset)}')\n",
    "#print(f'Number of train batches: {len(train_dl)}, number of test batches: {len(test_dl)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making modell, loss function and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dev\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\Dev\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#name = 'resnet'\n",
    "#modell = model(name)\n",
    "#optimizer = torch.optim.Adam(modell.parameters(), lr=3e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, loss_fn, optimizer, train_dataset, test_dataset, batch_size, shuffle ):\n",
    "    \"\"\"\n",
    "        A simple train function \n",
    "        Params: \n",
    "            epoch: number of epochs to train for\n",
    "    \"\"\"\n",
    "    train_dl = DataLoader(train_dataset, batch_size = batch_size, shuffle= shuffle)\n",
    "    test_dl = DataLoader(test_dataset, batch_size = batch_size, shuffle= shuffle)\n",
    "\n",
    "    history = {'train_loss': [],\n",
    "               'train_accuracy': [],\n",
    "               'val_loss': [],\n",
    "               'val_accuracy': []}\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "    else: \n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    print('Starting training..')\n",
    "    for e in range(epochs):\n",
    "        print('='*20)\n",
    "        print(f'Starting epoch {e + 1}/{epochs}')\n",
    "        print('='*20)\n",
    "\n",
    "        train_loss = 0.\n",
    "        train_accuracy = 0.\n",
    "\n",
    "        model.train() # set model to training phase\n",
    "\n",
    "        for train_step, (images, labels) in enumerate(train_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_accuracy += sum((preds == labels).numpy())\n",
    "            if train_step%20==0:\n",
    "                print(f\"Training round {train_step}\")\n",
    "\n",
    "        train_loss /= (train_step + 1)\n",
    "        train_accuracy = train_accuracy/len(train_dataset)\n",
    "        print(f'Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "        val_loss = 0.\n",
    "        val_accuracy = 0.0\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        for val_step, (images, labels) in enumerate(test_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_accuracy += sum((preds == labels).numpy())\n",
    "\n",
    "        val_loss /= (val_step + 1)\n",
    "        val_accuracy = val_accuracy/len(test_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        model.train()\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    print('Training complete..')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(model, history, split, data_ratio, mode):\n",
    "    FILEBASE = f\"{model.get_name}_model_{split}_split_{data_ratio}_ratio_{mode}_mode\"\n",
    "    torch.save(model.state_dict(), FILEBASE + '.pt')\n",
    "    with open(FILEBASE + '-history.pkl', 'wb') as file:\n",
    "        pickle.dump(file, history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dev\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\Dev\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/1\n",
      "====================\n",
      "Training round 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13856\\1194770956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mOPTIMIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0.00003\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOSS_FN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mSHUFFLE\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0msave_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSPLIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDATA_RATIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13856\\2255764221.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, model, loss_fn, optimizer, train_dataset, test_dataset, batch_size, shuffle)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set model to training phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Dev\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Dev\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Dev\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Dev\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13856\\3680720302.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Dev\\Anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    899\u001b[0m         \"\"\"\n\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transparency\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Dev\\Anaconda\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exclusive_fp\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_exclusive_fp_after_loading\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#'orig' and '0' through '3'\n",
    "#'oversampling', 'gan'\n",
    "# # 1, 0.8, 0.6, 0.4, 0.2\n",
    "#'transform' : None, \n",
    "#'geoaugment': False,\n",
    "#'seed'      : 0\n",
    "\n",
    "#dataset rules\n",
    "SPLIT = 'orig'\n",
    "MODE = None\n",
    "DATA_RATIO=0.2\n",
    "GEOAUGMENT = False\n",
    "\n",
    "#Network\n",
    "NETWORK = 'resnet'\n",
    "EPOCHS = 1\n",
    "SHUFFLE=True\n",
    "BATCH=64\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, test_dataset = DatasetMaker(split = SPLIT, mode = MODE, data_ratio = DATA_RATIO, geoaugment = GEOAUGMENT) #, params['transform'], params['geoaugment'], params['seed']) \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size= BATCH, shuffle = True)\n",
    "test_dl = DataLoader(test_dataset, batch_size= BATCH, shuffle = True )\n",
    "\n",
    "model = model(NETWORK)\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(), lr=3e-5) #0.00003\n",
    "\n",
    "model, history = train(EPOCHS, model, LOSS_FN, OPTIMIZER, train_dataset, test_dataset, BATCH,  SHUFFLE )\n",
    "\n",
    "save_history(model, history, SPLIT, DATA_RATIO, MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset rules\n",
    "SPLIT = 'orig'\n",
    "MODE = None\n",
    "DATA_RATIO=1\n",
    "GEOAUGMENT = False\n",
    "\n",
    "#Network\n",
    "NETWORK = 'resnet'\n",
    "EPOCHS = 1\n",
    "SHUFFLE=True\n",
    "\n",
    "train_dataset, test_dataset = DatasetMaker(split = SPLIT, mode = MODE, data_ratio = DATA_RATIO, geoaugment = GEOAUGMENT) #, params['transform'], params['geoaugment'], params['seed']) \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size= BATCH, shuffle = True)\n",
    "test_dl = DataLoader(test_dataset, batch_size= BATCH, shuffle = True )\n",
    "\n",
    "model = model(NETWORK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcb96da45b2fc45e59405d102e32af3d42527a73937a6435f97ad1b01889c6a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
