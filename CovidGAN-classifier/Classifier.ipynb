{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing GAN generated data augmentation, using different proportions of data for training GAN \n",
    "## This was \n",
    "\n",
    "Dataset is COVID_QU_Ex dataset from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import subprocess\n",
    "import time\n",
    "import neptune \n",
    "import matplotlib\n",
    "from sklearn.metrics  import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, balanced_accuracy_score, recall_score, precision_score, f1_score\n",
    "import gc\n",
    "#This code needs a little bit rework, so testing will be easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = 'Lung_Segmentation_Data'\n",
    "#run = neptune.init_run()\n",
    "\n",
    "torch.set_num_threads(12)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_directories = {\n",
    "    'Test_orig_0.8' : {'dir':'2023-03-14_19-00-08','best':'127.0.0.1-5002'},\n",
    "    'Test_orig_0.6' : {'dir':'2023-03-15_09-51-22','best':'127.0.0.1-5001'},\n",
    "    'Test_orig_0.4' : {'dir':'2023-03-16_17-23-13','best':'127.0.0.1-5002'},\n",
    "    'Test_orig_0.2' : {'dir':'2023-03-17_11-28-34','best':'127.0.0.1-5002'},\n",
    "    \n",
    "    'Test_0_0.8' : {'dir':'2023-03-23_15-19-22','best':'127.0.0.1-5002'},\n",
    "    'Test_0_0.6' : {'dir':'2023-03-24_16-23-49','best':'127.0.0.1-5001'},\n",
    "    'Test_0_0.4' : {'dir':'2023-03-25_09-50-59','best':'127.0.0.1-5001'},\n",
    "    'Test_0_0.2' : {'dir':'2023-03-25_16-58-45','best':'127.0.0.1-5003'},\n",
    "\n",
    "    'Test_1_0.8' : {'dir':'2023-03-21_16-13-32','best':'127.0.0.1-5003'},\n",
    "    'Test_1_0.6' : {'dir':'2023-03-22_09-56-24','best':'127.0.0.1-5001'},\n",
    "    'Test_1_0.4' : {'dir':'2023-03-22_18-38-27','best':'127.0.0.1-5000'},\n",
    "    'Test_1_0.2' : {'dir':'2023-03-23_08-16-51','best':'127.0.0.1-5002'},\n",
    "\n",
    "    'Test_2_0.8' : {'dir':'2023-03-19_18-20-08','best':'127.0.0.1-5001'},\n",
    "    'Test_2_0.6' : {'dir':'2023-03-20_08-46-21','best':'127.0.0.1-5001'},\n",
    "    'Test_2_0.4' : {'dir':'2023-03-20_16-58-27','best':'127.0.0.1-5001'},\n",
    "    'Test_2_0.2' : {'dir':'2023-03-21_08-49-59','best':'127.0.0.1-5002'},\n",
    "\n",
    "    'Test_3_0.8' : {'dir':'2023-03-17_17-42-29','best':'127.0.0.1-5001'},\n",
    "    'Test_3_0.6' : {'dir':'2023-03-18_07-56-44','best':'127.0.0.1-5003'},\n",
    "    'Test_3_0.4' : {'dir':'2023-03-18_23-01-23','best':'127.0.0.1-5002'},\n",
    "    'Test_3_0.2' : {'dir':'2023-03-19_07-43-42','best':'127.0.0.1-5000'},\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_\n",
    "# {params['augment']}_{params['geoaugment']}_{params['id']}\"\n",
    "# f\"{split}_{network_name}_{data_ratio}_{augment}_{geoaugment}\"\n",
    "case_to_neptune_id = {\n",
    "    \"orig_resnet_1_None_False\" : \"COV1-80\",\n",
    "    \"orig_resnet_1_None_True\" : \"COV1-88\",\n",
    "    \n",
    "    \"orig_resnet_0.8_None_False\" : \"COV1-60\",\n",
    "    \"orig_resnet_0.8_None_True\" : \"COV1-225\",\n",
    "    \"orig_resnet_0.8_oversampling_False\" : \"COV1-226\",\n",
    "    \"orig_resnet_0.8_oversampling_True\" : \"COV1-228\",\n",
    "    \"orig_resnet_0.8_gan_False\" : \"COV1-273\",\n",
    "    \"orig_resnet_0.8_gan_True\" : \"COV1-274\",\n",
    "    \n",
    "    \"orig_resnet_0.6_None_False\" : \"COV1-68\",\n",
    "    \"orig_resnet_0.6_None_True\" : \"COV1-264\",\n",
    "    \"orig_resnet_0.6_oversampling_False\" : \"COV1-275\",\n",
    "    \"orig_resnet_0.6_oversampling_True\" : \"COV1-250\",\n",
    "    \"orig_resnet_0.6_gan_False\" : \"COV1-276\",\n",
    "    \"orig_resnet_0.6_gan_True\" : \"COV1-277\",\n",
    "    \n",
    "    \"orig_resnet_0.4_None_False\" : \"COV1-72\",\n",
    "    \"orig_resnet_0.4_None_True\" : \"COV1-278\",\n",
    "    \"orig_resnet_0.4_oversampling_False\" : \"COV1-125\",\n",
    "    \"orig_resnet_0.4_oversampling_True\" : \"COV1-245\",\n",
    "    \"orig_resnet_0.4_gan_False\" : \"COV1-279\",\n",
    "    \"orig_resnet_0.4_gan_True\" : \"COV1-280\",\n",
    "    \n",
    "    \"orig_resnet_0.2_None_False\" : \"COV1-86\",\n",
    "    \"orig_resnet_0.2_None_True\" : \"COV1-101\",\n",
    "    \"orig_resnet_0.2_oversampling_False\" : \"COV1-251\",\n",
    "    \"orig_resnet_0.2_oversampling_True\" : \"COV1-281\",\n",
    "    \"orig_resnet_0.2_gan_False\" : \"COV1-269\",\n",
    "    \"orig_resnet_0.2_gan_True\" : \"COV1-282\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "        A simple imagedatset for storing data\n",
    "    \"\"\"\n",
    "    #Imagefolder for efficiency\n",
    "    def __init__(self, images, transform):\n",
    "        target_dir = os.path.join(ROOT_DIR, 'original')\n",
    "        super().__init__(target_dir, transform)\n",
    "        self.samples = images\n",
    "        self.imgs = images\n",
    "\n",
    "def DatasetMaker(split, mode=None, data_ratio=1, transform = None, geoaugment=False, random_affine = 0, seed = 0):\n",
    "    \"\"\"\n",
    "        Returns a CustomDataset with given parameters\n",
    "        split: str, determines train-test to use; options: 'orig', '0', '1', '2', '3'\n",
    "        mode: str, 'oversampling', 'gan' or None\n",
    "            'oversampling' : oversample with real images to balance classes\n",
    "                    'gan' : balance datasets with gan generated images (uses data_ratio to figure out which gan to use)\n",
    "                    None  : dataset won't be balanced\n",
    "        data_ratio (optional): int, the ratio of the training covid data to be used\n",
    "                    options:\n",
    "                        '1' : all data\n",
    "                        '0.8' : 80% of training images\n",
    "                        '0.6' : 60% of training images\n",
    "                        '0.4' : 40% of training images\n",
    "                        '0.2' : 20% of training images\n",
    "        transform (optional): torch.Compose instance, sets the dataset's transforms\n",
    "        geougment (optional): bool, uses basic data augmentation techmiques\n",
    "        seed (optional): int, seed to use for reproducibility\n",
    "    \"\"\"\n",
    "    #This code prepares my fixed set of images to replace the Imagefolder's original images\n",
    "    #Maybe this could be done nicer with overwriting the DatasetFolder's find_classes method, but currently this works\n",
    "    # the idea is to make (route, index) pairs for later loading in images\n",
    "    classes = ['normal', 'viral', 'covid']\n",
    "\n",
    "    #Determining the root directories for original images \n",
    "    orig_dirs = {\n",
    "    'normal' : f'{ROOT_DIR}/original/Normal',\n",
    "    'viral' : f'{ROOT_DIR}/original/Non-Covid',\n",
    "    'covid' : f'{ROOT_DIR}/original/COVID-19'\n",
    "    }\n",
    "    #Determining the root directories for generated images \n",
    "    fake_dirs = {\n",
    "        'gan_0.8' : f'{ROOT_DIR}/generated/Test_{split}/gan_0.8',\n",
    "        'gan_0.6' : f'{ROOT_DIR}/generated/Test_{split}/gan_0.6',\n",
    "        'gan_0.4' : f'{ROOT_DIR}/generated/Test_{split}/gan_0.4',\n",
    "        'gan_0.2' : f'{ROOT_DIR}/generated/Test_{split}/gan_0.2'\n",
    "    }\n",
    "    #Determining the root directories for files which contain the names of the COVID19 pictures \n",
    "    indicies_files = {\n",
    "        'gan_0.8' : f'{ROOT_DIR}/Indicies_files/Test_{split}/{split}_split_0.8_gan.pkl', \n",
    "        'gan_0.6' : f'{ROOT_DIR}/Indicies_files/Test_{split}/{split}_split_0.6_gan.pkl',\n",
    "        'gan_0.4' : f'{ROOT_DIR}/Indicies_files/Test_{split}/{split}_split_0.4_gan.pkl',\n",
    "        'gan_0.2' : f'{ROOT_DIR}/Indicies_files/Test_{split}/{split}_split_0.2_gan.pkl',\n",
    "        'test'  : f'{ROOT_DIR}/Indicies_files/Test_{split}/{split}_split_test.pkl',\n",
    "        'train'  : f'{ROOT_DIR}/Indicies_files/Test_{split}/{split}_split_train_and_val.pkl'\n",
    "    }\n",
    "    class_idx = {\n",
    "        'covid': 0, \n",
    "        'viral': 1,\n",
    "        'normal': 2,\n",
    "        'gan_0.8' : 0,\n",
    "        'gan_0.6' : 0,\n",
    "        'gan_0.4' : 0,\n",
    "        'gan_0.2' : 0\n",
    "    }\n",
    "    idx_to_class ={\n",
    "        0: 'covid',\n",
    "        1: 'viral',\n",
    "        2: 'normal'\n",
    "    }\n",
    "    valid_ratios = [1, 0.8, 0.6, 0.4, 0.2]\n",
    "\n",
    "    #The dictionary that will contain the route for the several image classes\n",
    "    source_dir = {}\n",
    "    train_images = []\n",
    "\n",
    "    #creating the sources for the classes\n",
    "    for class_name in classes: \n",
    "        source_dir[class_name] = orig_dirs[class_name]\n",
    "    \n",
    "    #Get all training images\n",
    "    file = indicies_files['train'] \n",
    "    imgs = load_images_from_file(file)\n",
    "    imgs = [*imgs[0],*imgs[1]] #The file contains (train, val) sets\n",
    "    \n",
    "    #Making the paths for the training images\n",
    "    num_of_covid_imgs = 0 #?\n",
    "    for x in imgs: \n",
    "        class_of_x = idx_to_class[x[1]] #The images are saved in (image_name, class_index) format\n",
    "        item = os.path.join(source_dir[class_of_x],x[0]),class_idx[class_of_x] \n",
    "        if item[1] == class_idx['covid']: num_of_covid_imgs +=1 #?\n",
    "        train_images.append(item)\n",
    "    #If data_ratio is not 1, we need to change the covid images of the dataset\n",
    "    if data_ratio!=1:\n",
    "        num_of_covid_imgs = 0\n",
    "        train_images = [x for x in train_images if x[1]!=class_idx['covid']]\n",
    "        file = indicies_files[f'gan_{data_ratio}']\n",
    "        imgs = load_images_from_file(file)\n",
    "        for x in imgs:\n",
    "            class_of_x = idx_to_class[x[1]]\n",
    "            item = os.path.join(source_dir[class_of_x],x[0]),class_idx[class_of_x] \n",
    "            if item[1] == class_idx['covid']: num_of_covid_imgs +=1 \n",
    "            train_images.append(item)\n",
    "    \n",
    "    #If picture generation is needed we want to know the average class size and missing number of covid images\n",
    "    average_class_size = (len(train_images)-num_of_covid_imgs)//2\n",
    "    missing_images = max(0, average_class_size - num_of_covid_imgs)\n",
    "\n",
    "    #Determining wether image generation is needed and which kind of it then making the generation\n",
    "    if mode=='gan' and (data_ratio in valid_ratios):\n",
    "        gan = f'gan_{data_ratio}'\n",
    "        gan_dir = fake_dirs[gan]\n",
    "        #generate_images_to_dir(split, data_ratio, gan, gan_dir, missing_images) #?\n",
    "        gan_ims = []\n",
    "        for x in os.listdir(gan_dir): #optimize further\n",
    "            if x.lower().endswith('jpg'):\n",
    "                item = os.path.join(gan_dir, x), class_idx['covid']\n",
    "                gan_ims.append(item)\n",
    "        sample = random.sample(gan_ims, missing_images)\n",
    "        train_images = [*train_images,*sample]\n",
    "    elif mode=='oversampling':\n",
    "        covid_images = [x for x in train_images if x[1]==class_idx['covid']]\n",
    "        batch_size = len(covid_images)\n",
    "        while batch_size <= missing_images:\n",
    "            train_images = [*train_images, *covid_images]\n",
    "            missing_images -= batch_size\n",
    "        if missing_images>0:\n",
    "            sample = random.sample(covid_images, missing_images)\n",
    "            train_images = [*train_images, *sample]\n",
    "    \n",
    "    #Making the paths for the test images\n",
    "    test_images = []\n",
    "    test_imgs = load_images_from_file(indicies_files['test'])\n",
    "    for x in test_imgs:\n",
    "        class_of_x = idx_to_class[x[1]]\n",
    "        item = os.path.join(source_dir[class_of_x],x[0]),class_idx[class_of_x] #This should be correct\n",
    "        test_images.append(item)\n",
    "    random.Random(seed).shuffle(test_images)\n",
    "    bound = len(test_images)//2\n",
    "    val_images = test_images[bound:]\n",
    "    test_images = test_images[:bound]\n",
    "    #Making some basic transforms \n",
    "    if transform is None:\n",
    "        transforms = [torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.Grayscale(num_output_channels=1)]\n",
    "    else:\n",
    "        transforms = transform\n",
    "\n",
    "    #Checking wether geometric augmentation is needed ( classic augmentation methods)\n",
    "    if geoaugment:\n",
    "        augmentation_transforms = [torchvision.transforms.RandomAffine(random_affine),\n",
    "                            torchvision.transforms.Resize(size=(140,140)),\n",
    "                                   #torchvision.transforms.RandomCrop(size = (128,128))]\n",
    "                            torchvision.transforms.FiveCrop(size = (128,128)),\n",
    "                            torchvision.transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))]\n",
    "        transforms = [*transforms,*augmentation_transforms]\n",
    "    else:\n",
    "        size_transform = [torchvision.transforms.Resize(size=(128, 128))]\n",
    "        transforms = [*transforms,*size_transform]\n",
    "    transforms = torchvision.transforms.Compose(transforms)\n",
    "    train_dataset = CustomDataset(train_images, transforms)\n",
    "    val_dataset = CustomDataset(val_images, transforms)\n",
    "    test_dataset = CustomDataset(test_images,  transforms)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def load_images_from_file(file):\n",
    "    with open(file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data   \n",
    "    \n",
    "#def generate_images_to_dir(split, data_ratio, gan, directory, size):\n",
    "#    \"\"\"\n",
    "#        Generates pictures with a given gan, to a given directory\n",
    "#    \"\"\"\n",
    "#    #Goes into Lipizzaner's directory and then generates images with a given GAN (this function is a Lipizzaner built-in method)\n",
    "#    #Then returns into this directory\n",
    "#    \n",
    "#    curr_dir = os.getcwd()\n",
    "#    print(curr_dir)\n",
    "#    \n",
    "#    lippi_dir = '/home/bbernard/lipizzaner-covidgan-master/src/'  #Change this on server\n",
    "#    \n",
    "#    output_dir = os.path.join(curr_dir, directory) #?\n",
    "#   \n",
    "#   #Gan to use is determined by the split and the data_ratio parameters\n",
    "#    gan_dir = gan_directories[f'Test_{split}_{data_ratio}'] \n",
    "#    src_dir = os.path.join(lippi_dir, f'output/lipizzaner_gan/master/{gan_dir}/127.0.0.1-5000')\n",
    "#    \n",
    "#    config_file = os.path.join(lippi_dir, f'configuration/covid-qu-conv/Test_{split}/covidqu_{data_ratio}.yml')\n",
    "#    \n",
    "#    #man = os.path.join(lippi_dir, 'main.py')#\n",
    "#\n",
    "#    code =f'python main.py generate --mixture-source {src_dir} -o {output_dir} --sample-size {size} -f {config_file}'\n",
    "#    os.chdir(lippi_dir)\n",
    "#    subprocess.run(code, shell=True)\n",
    "#    os.chdir(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name):\n",
    "    \"\"\"\n",
    "        Returns pretrained models\n",
    "    \"\"\"\n",
    "    if name==\"resnet\":\n",
    "        resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        resnet18.conv1= torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        resnet18.fc = torch.nn.Linear(in_features=512, out_features=3)\n",
    "        resnet18.get_name = 'resnet18'\n",
    "        return resnet18\n",
    "    elif name==\"vgg\":\n",
    "        vgg16 = torchvision.models.vgg16(weights = torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        vgg16.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        vgg16.classifier[6] = torch.nn.Linear(in_features=4096, out_features=3, bias=True)\n",
    "        vgg16.get_name = 'vgg16'\n",
    "        return vgg16\n",
    "    elif name==\"efficient\":\n",
    "        efficient = torchvision.models.efficientnet_b0(weights=torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        efficient.features[0][0] = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        efficient.classifier[1] = torch.nn.Linear(in_features=1280, out_features=3, bias=True)\n",
    "        efficient.get_name = 'efficientnet_b0'\n",
    "        return efficient\n",
    "    else:\n",
    "        print(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, loss_fn, optimizer, weight_decay, train_dataset, test_dataset, batch_size, shuffle, neptune_run): #, neptune_run ):\n",
    "    \"\"\"\n",
    "        A simple train function \n",
    "        Params: \n",
    "            epoch: number of epochs to train for\n",
    "            modell: The neural network to train\n",
    "            loss_fn: Loss function instance\n",
    "            optimizer: optimizer instance\n",
    "            train_dataset (Dataset)\n",
    "            test_dataset (Dataset)\n",
    "            batch_size (int)\n",
    "            shuffle (bool) \n",
    "    \"\"\"\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, num_workers=0, shuffle= shuffle)\n",
    "    test_dl =  torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, num_workers=0, shuffle= shuffle)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=weight_decay)\n",
    "    print('Starting training..')\n",
    "    net.to(device)\n",
    "    for e in range(epochs):\n",
    "        print('='*20)\n",
    "        print(f'Starting epoch {e + 1}/{epochs}')\n",
    "        print('='*20)\n",
    "\n",
    "        train_iter = iter(train_dl)\n",
    "        \n",
    "        train_accuracy = 0.\n",
    "        train_loss = 0.\n",
    "        \n",
    "        sample_num = 0 \n",
    "\n",
    "        net.train() #set model to training phase\n",
    "        \n",
    "        #Training \n",
    "        batch_num = 0\n",
    "        \n",
    "        while batch_num< len(train_dl):\n",
    "            images, labels = next(train_iter)\n",
    "            if len(images.size())==5:\n",
    "                bs, ncrops, c, h, w = images.size()\n",
    "            else:\n",
    "                bs, c, h, w = images.size()\n",
    "                ncrops = 1\n",
    "            images = images.view(-1, c,h,w)\n",
    "            images = images.to(device) \n",
    "            \n",
    "            labels = labels.to(device)\n",
    "            #labels = labels.repeat_interleave(ncrops)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            outputs = outputs.view(bs, ncrops, -1).mean(1) \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #train_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            sample_num += len(labels)\n",
    "            train_accuracy += sum((preds == labels).cpu().numpy())\n",
    "            train_loss += loss.item()\n",
    "            batch_num += 1\n",
    "        \n",
    "        train_loss/= len(train_dl)\n",
    "        train_accuracy/= len(train_dataset)\n",
    "        print(f'Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "        neptune_run['train/train_loss'].append(train_loss)\n",
    "        neptune_run['train/train_accuracy'].append(train_accuracy)\n",
    "\n",
    "        val_loss = 0.\n",
    "        val_accuracy = 0.\n",
    "\n",
    "        val_batch_num = 0\n",
    "        val_iter = iter(test_dl)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        net.eval()\n",
    "        while val_batch_num < len(val_iter):\n",
    "            images, labels = next(val_iter)\n",
    "            if len(images.size())==5:\n",
    "                bs, ncrops, c, h, w = images.size()\n",
    "            else:\n",
    "                bs, c, h, w = images.size()\n",
    "                ncrops = 1\n",
    "            \n",
    "            images = images.view(-1, c,h,w)\n",
    "            images = images.to(device) \n",
    "            \n",
    "            #labels = labels.repeat_interleave(ncrops)\n",
    "            y_true.extend(labels)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            outputs = outputs.view(bs, ncrops, -1).mean(1) \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_pred.extend(preds.detach().cpu())\n",
    "            val_accuracy += sum((preds == labels).cpu().numpy())\n",
    "            val_batch_num += 1\n",
    "\n",
    "        val_loss /= len(test_dl)\n",
    "        val_accuracy = val_accuracy/len(test_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "        scheduler.step()\n",
    "        net.train()\n",
    "\n",
    "        neptune_run['train/val_loss'].append(val_loss)\n",
    "        neptune_run['train/val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        curr_conf_matrix = confusion_matrix(y_true, y_pred) \n",
    "        curr_conf_matrix = curr_conf_matrix / np.sum(curr_conf_matrix)\n",
    "        im = ConfusionMatrixDisplay(curr_conf_matrix, display_labels=[\"covid\", \"viral\",\"normal\"]).plot()\n",
    "        neptune_run['metrics/conf_matrix'].append(im.figure_) #, description=f\"Confusion matrix in the iteration: {iteration}\"  File.as_image(curr_conf_matrix))\n",
    "        matplotlib.pyplot.close()\n",
    "        neptune_run['metrics/acc'].append( accuracy_score(y_true, y_pred))\n",
    "        neptune_run['metrics/bal_acc'].append(balanced_accuracy_score(y_true, y_pred))\n",
    "        neptune_run['metrics/recall'].append(recall_score(y_true, y_pred,average =\"micro\"))\n",
    "        neptune_run['metrics/precision'].append( precision_score(y_true, y_pred, average =\"micro\"))\n",
    "        neptune_run['metrics/f1'].append( f1_score(y_true, y_pred,average =\"micro\"))    \n",
    "        #neptune_run['val/loss'].append(val_loss)\n",
    "        #neptune_run['val/accuracy'].append(val_accuracy)\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "    print('Training complete..')\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(network,folder, case_string): #,history\n",
    "    \"\"\"\n",
    "        A simple function that saves the histories and the clasificator net\n",
    "    \"\"\"\n",
    "    FILEBASE = f\"Histories/{folder}/{case_string}\"\n",
    "    torch.save(network.state_dict(), FILEBASE + '.pt')\n",
    "    #with open(FILEBASE + '-history.pkl', 'wb') as file:\n",
    "    #    pickle.dump(history, file)\n",
    "    #    print(f'{FILEBASE} instance saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_in_network(network_name, case):\n",
    "    network = get_model(network_name)\n",
    "    network.load_state_dict(torch.load(case))\n",
    "    return network\n",
    "\n",
    "def test_run(split, data_ratio, augment, geoaugment, network_name):\n",
    "    neptune_id = case_to_neptune_id[f\"{split}_{network_name}_{data_ratio}_{augment}_{geoaugment}\"]\n",
    "    neptune_run = neptune_init_run(with_id = neptune_id)\n",
    "    if network_name==\"resnet\":\n",
    "        case_id = neptune_run[\"params/id\"].fetch()\n",
    "        try:\n",
    "            degree == neptune_run[\"params/degree\"].fetch()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "            degree = 4\n",
    "            neptune_run[\"params/degree\"] = degree\n",
    "    else:\n",
    "        case_id = neptune_id #bad logging practice\n",
    "    \n",
    "    case_string = f\"{network_name}_{split}_{data_ratio}_{augment}_{geoaugment}_{case_id}\"\n",
    "    network = load_in_network(network_name, case_string)\n",
    "    _,_,test_dataset = DatasetMaker(split = split,mode = augment, \n",
    "                                data_ratio = data_ratio, geoaugment = geoaugment, random_affine = degree)\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    batch_size = 1024 #for now\n",
    "    \n",
    "    test_dl =  torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, num_workers=0)\n",
    "    test_iter = iter(test_dl)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_accuracy = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_batch_num = 0\n",
    "    \n",
    "    network.eval()\n",
    "    while test_batch_num < len(test_iter):\n",
    "        images, labels = next(test_iter)\n",
    "        if len(images.size())==5:\n",
    "            bs, ncrops, c, h, w = images.size()\n",
    "        else:\n",
    "            bs, c, h, w = images.size()\n",
    "            ncrops = 1\n",
    "\n",
    "        images = images.view(-1, c,h,w)\n",
    "        images = images.to(device) \n",
    "\n",
    "        #labels = labels.repeat_interleave(ncrops)\n",
    "        y_true.extend(labels)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = network(images)\n",
    "        outputs = outputs.view(bs, ncrops, -1).mean(1) \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_pred.extend(preds.detach().cpu())\n",
    "        test_accuracy += sum((preds == labels).cpu().numpy())\n",
    "        test_batch_num += 1\n",
    "\n",
    "    test_loss /= len(test_dl)\n",
    "    test_accuracy = test_accuracy/len(test_dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    neptune_run['test/test_loss'].append(test_loss)\n",
    "    neptune_run['test/test_accuracy'].append(test_accuracy)\n",
    "\n",
    "    curr_conf_matrix = confusion_matrix(y_true, y_pred) \n",
    "    curr_conf_matrix = curr_conf_matrix / np.sum(curr_conf_matrix)\n",
    "    im = ConfusionMatrixDisplay(curr_conf_matrix, display_labels=[\"covid\", \"viral\",\"normal\"]).plot()\n",
    "    neptune_run['test/metrics/conf_matrix'].append(im.figure_) #, description=f\"Confusion matrix in the iteration: {iteration}\"  File.as_image(curr_conf_matrix))\n",
    "    matplotlib.pyplot.close()\n",
    "    neptune_run['test/metrics/acc'].append( accuracy_score(y_true, y_pred))\n",
    "    neptune_run['test/metrics/bal_acc'].append(balanced_accuracy_score(y_true, y_pred))\n",
    "    neptune_run['test/metrics//recall'].append(recall_score(y_true, y_pred,average =\"micro\"))\n",
    "    neptune_run['test/metrics/precision'].append( precision_score(y_true, y_pred, average =\"micro\"))\n",
    "    neptune_run['test/metrics/f1'].append( f1_score(y_true, y_pred,average =\"micro\"))    \n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "#Maybe save the individual test results?\n",
    "#Somehow the data needs to be collectable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing classificators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run(\"orig\",1,\"None\",\"False\",\"resnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_run(split, data_ratio, augment, geoaugment, network_name)\n",
    "split = \"orig\"\n",
    "network = \"resnet\"\n",
    "ratios = [1, 0.8, 0.6, 0.4, 0.2]\n",
    "augments = [\"None\", \"oversampling\", \"gan\"]\n",
    "geoaugments = [\"False\", \"True\"]\n",
    "\n",
    "for ratio in ratios:\n",
    "    for geo in geoaugments:\n",
    "        if ratio!=1:\n",
    "            for augment in augments:\n",
    "                test_run(split,ratio,augment,geo,network)\n",
    "        else:\n",
    "            augment = \"None\"\n",
    "            test_run(split,ratio,augment,geo,network)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/1\n",
      "====================\n",
      "Training Loss: 0.4578, Accuracy: 0.8145\n",
      "Validation Loss: 0.2948, Accuracy: 0.8922\n",
      "Training complete..\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict #neptune_run = defaultdict(list) #\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 1,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'None',       #gan, oversampling, None\n",
    "'geoaugment': True,\n",
    "'network_name' : \"vgg\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 32,\n",
    "'epochs' : 1, #30/40\n",
    "'adam_lr' : 3e-5,\n",
    "'adam_w' :0.1,\n",
    "'weight_decay':0.1,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamw\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_w',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = defaultdict(list) #neptune.init_run() \n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "#neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{params['id']}\"\n",
    "save_history(net,params['network_name'], case_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2702673/459653350.py:28: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  neptune_run = neptune.init_run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/aielte/CovidGAN/e/COV1-439\n",
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/30\n",
      "====================\n",
      "Training Loss: 0.4652, Accuracy: 0.8150\n",
      "Validation Loss: 0.2165, Accuracy: 0.9225\n",
      "====================\n",
      "Starting epoch 2/30\n",
      "====================\n",
      "Training Loss: 0.2678, Accuracy: 0.8966\n",
      "Validation Loss: 0.1711, Accuracy: 0.9405\n",
      "====================\n",
      "Starting epoch 3/30\n",
      "====================\n",
      "Training Loss: 0.2131, Accuracy: 0.9172\n",
      "Validation Loss: 0.1530, Accuracy: 0.9428\n",
      "====================\n",
      "Starting epoch 4/30\n",
      "====================\n",
      "Training Loss: 0.1701, Accuracy: 0.9342\n",
      "Validation Loss: 0.1529, Accuracy: 0.9458\n",
      "====================\n",
      "Starting epoch 5/30\n",
      "====================\n",
      "Training Loss: 0.1419, Accuracy: 0.9454\n",
      "Validation Loss: 0.1699, Accuracy: 0.9437\n",
      "====================\n",
      "Starting epoch 6/30\n",
      "====================\n",
      "Training Loss: 0.0960, Accuracy: 0.9651\n",
      "Validation Loss: 0.1512, Accuracy: 0.9499\n",
      "====================\n",
      "Starting epoch 7/30\n",
      "====================\n",
      "Training Loss: 0.0804, Accuracy: 0.9708\n",
      "Validation Loss: 0.1469, Accuracy: 0.9496\n",
      "====================\n",
      "Starting epoch 8/30\n",
      "====================\n",
      "Training Loss: 0.0763, Accuracy: 0.9735\n",
      "Validation Loss: 0.1538, Accuracy: 0.9508\n",
      "====================\n",
      "Starting epoch 9/30\n",
      "====================\n",
      "Training Loss: 0.0687, Accuracy: 0.9755\n",
      "Validation Loss: 0.1520, Accuracy: 0.9517\n",
      "====================\n",
      "Starting epoch 10/30\n",
      "====================\n",
      "Training Loss: 0.0641, Accuracy: 0.9783\n",
      "Validation Loss: 0.1567, Accuracy: 0.9502\n",
      "====================\n",
      "Starting epoch 11/30\n",
      "====================\n",
      "Training Loss: 0.0592, Accuracy: 0.9802\n",
      "Validation Loss: 0.1575, Accuracy: 0.9514\n",
      "====================\n",
      "Starting epoch 12/30\n",
      "====================\n",
      "Training Loss: 0.0583, Accuracy: 0.9805\n",
      "Validation Loss: 0.1540, Accuracy: 0.9502\n",
      "====================\n",
      "Starting epoch 13/30\n",
      "====================\n",
      "Training Loss: 0.0575, Accuracy: 0.9804\n",
      "Validation Loss: 0.1602, Accuracy: 0.9508\n",
      "====================\n",
      "Starting epoch 14/30\n",
      "====================\n",
      "Training Loss: 0.0593, Accuracy: 0.9792\n",
      "Validation Loss: 0.1623, Accuracy: 0.9490\n",
      "====================\n",
      "Starting epoch 15/30\n",
      "====================\n",
      "Training Loss: 0.0576, Accuracy: 0.9796\n",
      "Validation Loss: 0.1746, Accuracy: 0.9481\n",
      "====================\n",
      "Starting epoch 16/30\n",
      "====================\n",
      "Training Loss: 0.0554, Accuracy: 0.9807\n",
      "Validation Loss: 0.1676, Accuracy: 0.9473\n",
      "====================\n",
      "Starting epoch 17/30\n",
      "====================\n",
      "Training Loss: 0.0574, Accuracy: 0.9797\n",
      "Validation Loss: 0.1673, Accuracy: 0.9464\n",
      "====================\n",
      "Starting epoch 18/30\n",
      "====================\n",
      "Training Loss: 0.0569, Accuracy: 0.9811\n",
      "Validation Loss: 0.1591, Accuracy: 0.9496\n",
      "====================\n",
      "Starting epoch 19/30\n",
      "====================\n",
      "Training Loss: 0.0537, Accuracy: 0.9825\n",
      "Validation Loss: 0.1641, Accuracy: 0.9481\n",
      "====================\n",
      "Starting epoch 20/30\n",
      "====================\n",
      "Training Loss: 0.0573, Accuracy: 0.9811\n",
      "Validation Loss: 0.1607, Accuracy: 0.9499\n",
      "====================\n",
      "Starting epoch 21/30\n",
      "====================\n",
      "Training Loss: 0.0556, Accuracy: 0.9811\n",
      "Validation Loss: 0.1633, Accuracy: 0.9499\n",
      "====================\n",
      "Starting epoch 22/30\n",
      "====================\n",
      "Training Loss: 0.0583, Accuracy: 0.9801\n",
      "Validation Loss: 0.1600, Accuracy: 0.9484\n",
      "====================\n",
      "Starting epoch 23/30\n",
      "====================\n",
      "Training Loss: 0.0587, Accuracy: 0.9792\n",
      "Validation Loss: 0.1605, Accuracy: 0.9481\n",
      "====================\n",
      "Starting epoch 24/30\n",
      "====================\n",
      "Training Loss: 0.0566, Accuracy: 0.9809\n",
      "Validation Loss: 0.1656, Accuracy: 0.9484\n",
      "====================\n",
      "Starting epoch 25/30\n",
      "====================\n",
      "Training Loss: 0.0568, Accuracy: 0.9801\n",
      "Validation Loss: 0.1611, Accuracy: 0.9484\n",
      "====================\n",
      "Starting epoch 26/30\n",
      "====================\n",
      "Training Loss: 0.0562, Accuracy: 0.9807\n",
      "Validation Loss: 0.1869, Accuracy: 0.9478\n",
      "====================\n",
      "Starting epoch 27/30\n",
      "====================\n",
      "Training Loss: 0.0563, Accuracy: 0.9802\n",
      "Validation Loss: 0.1583, Accuracy: 0.9481\n",
      "====================\n",
      "Starting epoch 28/30\n",
      "====================\n",
      "Training Loss: 0.0526, Accuracy: 0.9830\n",
      "Validation Loss: 0.1604, Accuracy: 0.9502\n",
      "====================\n",
      "Starting epoch 29/30\n",
      "====================\n",
      "Training Loss: 0.0541, Accuracy: 0.9814\n",
      "Validation Loss: 0.1581, Accuracy: 0.9499\n",
      "====================\n",
      "Starting epoch 30/30\n",
      "====================\n",
      "Training Loss: 0.0572, Accuracy: 0.9795\n",
      "Validation Loss: 0.1610, Accuracy: 0.9493\n",
      "Training complete..\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 8 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 8 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/aielte/CovidGAN/e/COV1-439/metadata\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.8,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'oversampling',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 32,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.1,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/aielte/CovidGAN/e/COV1-473\n",
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/30\n",
      "====================\n",
      "Training Loss: 0.4176, Accuracy: 0.8310\n",
      "Validation Loss: 0.2061, Accuracy: 0.9246\n",
      "====================\n",
      "Starting epoch 2/30\n",
      "====================\n",
      "Training Loss: 0.2693, Accuracy: 0.8977\n",
      "Validation Loss: 0.1546, Accuracy: 0.9478\n",
      "====================\n",
      "Starting epoch 3/30\n",
      "====================\n",
      "Training Loss: 0.2240, Accuracy: 0.9128\n",
      "Validation Loss: 0.1523, Accuracy: 0.9408\n",
      "====================\n",
      "Starting epoch 4/30\n",
      "====================\n",
      "Training Loss: 0.1924, Accuracy: 0.9241\n",
      "Validation Loss: 0.1382, Accuracy: 0.9517\n",
      "====================\n",
      "Starting epoch 5/30\n",
      "====================\n",
      "Training Loss: 0.1687, Accuracy: 0.9337\n",
      "Validation Loss: 0.1239, Accuracy: 0.9552\n",
      "====================\n",
      "Starting epoch 6/30\n",
      "====================\n",
      "Training Loss: 0.1394, Accuracy: 0.9471\n",
      "Validation Loss: 0.1202, Accuracy: 0.9576\n",
      "====================\n",
      "Starting epoch 7/30\n",
      "====================\n",
      "Training Loss: 0.1281, Accuracy: 0.9505\n",
      "Validation Loss: 0.1178, Accuracy: 0.9585\n",
      "====================\n",
      "Starting epoch 8/30\n",
      "====================\n",
      "Training Loss: 0.1242, Accuracy: 0.9532\n",
      "Validation Loss: 0.1138, Accuracy: 0.9596\n",
      "====================\n",
      "Starting epoch 9/30\n",
      "====================\n",
      "Training Loss: 0.1205, Accuracy: 0.9542\n",
      "Validation Loss: 0.1291, Accuracy: 0.9543\n",
      "====================\n",
      "Starting epoch 10/30\n",
      "====================\n",
      "Training Loss: 0.1213, Accuracy: 0.9544\n",
      "Validation Loss: 0.1122, Accuracy: 0.9626\n",
      "====================\n",
      "Starting epoch 11/30\n",
      "====================\n",
      "Training Loss: 0.1164, Accuracy: 0.9554\n",
      "Validation Loss: 0.1130, Accuracy: 0.9623\n",
      "====================\n",
      "Starting epoch 12/30\n",
      "====================\n",
      "Training Loss: 0.1161, Accuracy: 0.9555\n",
      "Validation Loss: 0.1189, Accuracy: 0.9590\n",
      "====================\n",
      "Starting epoch 13/30\n",
      "====================\n",
      "Training Loss: 0.1145, Accuracy: 0.9555\n",
      "Validation Loss: 0.1156, Accuracy: 0.9573\n",
      "====================\n",
      "Starting epoch 14/30\n",
      "====================\n",
      "Training Loss: 0.1161, Accuracy: 0.9550\n",
      "Validation Loss: 0.1198, Accuracy: 0.9567\n",
      "====================\n",
      "Starting epoch 15/30\n",
      "====================\n",
      "Training Loss: 0.1141, Accuracy: 0.9570\n",
      "Validation Loss: 0.1174, Accuracy: 0.9582\n",
      "====================\n",
      "Starting epoch 16/30\n",
      "====================\n",
      "Training Loss: 0.1154, Accuracy: 0.9549\n",
      "Validation Loss: 0.1224, Accuracy: 0.9570\n",
      "====================\n",
      "Starting epoch 17/30\n",
      "====================\n",
      "Training Loss: 0.1131, Accuracy: 0.9564\n",
      "Validation Loss: 0.1165, Accuracy: 0.9585\n",
      "====================\n",
      "Starting epoch 18/30\n",
      "====================\n",
      "Training Loss: 0.1153, Accuracy: 0.9549\n",
      "Validation Loss: 0.1207, Accuracy: 0.9573\n",
      "====================\n",
      "Starting epoch 19/30\n",
      "====================\n",
      "Training Loss: 0.1183, Accuracy: 0.9549\n",
      "Validation Loss: 0.1184, Accuracy: 0.9602\n",
      "====================\n",
      "Starting epoch 20/30\n",
      "====================\n",
      "Training Loss: 0.1156, Accuracy: 0.9553\n",
      "Validation Loss: 0.1204, Accuracy: 0.9579\n",
      "====================\n",
      "Starting epoch 21/30\n",
      "====================\n",
      "Training Loss: 0.1139, Accuracy: 0.9558\n",
      "Validation Loss: 0.1194, Accuracy: 0.9599\n",
      "====================\n",
      "Starting epoch 22/30\n",
      "====================\n",
      "Training Loss: 0.1183, Accuracy: 0.9541\n",
      "Validation Loss: 0.1144, Accuracy: 0.9611\n",
      "====================\n",
      "Starting epoch 23/30\n",
      "====================\n",
      "Training Loss: 0.1152, Accuracy: 0.9558\n",
      "Validation Loss: 0.1169, Accuracy: 0.9582\n",
      "====================\n",
      "Starting epoch 24/30\n",
      "====================\n",
      "Training Loss: 0.1170, Accuracy: 0.9549\n",
      "Validation Loss: 0.1203, Accuracy: 0.9582\n",
      "====================\n",
      "Starting epoch 25/30\n",
      "====================\n",
      "Training Loss: 0.1169, Accuracy: 0.9548\n",
      "Validation Loss: 0.1108, Accuracy: 0.9620\n",
      "====================\n",
      "Starting epoch 26/30\n",
      "====================\n",
      "Training Loss: 0.1141, Accuracy: 0.9563\n",
      "Validation Loss: 0.1176, Accuracy: 0.9620\n",
      "====================\n",
      "Starting epoch 27/30\n",
      "====================\n",
      "Training Loss: 0.1154, Accuracy: 0.9550\n",
      "Validation Loss: 0.1154, Accuracy: 0.9596\n",
      "====================\n",
      "Starting epoch 28/30\n",
      "====================\n",
      "Training Loss: 0.1177, Accuracy: 0.9545\n",
      "Validation Loss: 0.1153, Accuracy: 0.9605\n",
      "====================\n",
      "Starting epoch 29/30\n",
      "====================\n",
      "Training Loss: 0.1155, Accuracy: 0.9550\n",
      "Validation Loss: 0.1176, Accuracy: 0.9593\n",
      "====================\n",
      "Starting epoch 30/30\n",
      "====================\n",
      "Training Loss: 0.1150, Accuracy: 0.9550\n",
      "Validation Loss: 0.1218, Accuracy: 0.9579\n",
      "Training complete..\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 2 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 2 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/aielte/CovidGAN/e/COV1-473/metadata\n",
      "https://app.neptune.ai/aielte/CovidGAN/e/COV1-474\n",
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/30\n",
      "====================\n",
      "Training Loss: 0.3858, Accuracy: 0.8497\n",
      "Validation Loss: 0.1980, Accuracy: 0.9322\n",
      "====================\n",
      "Starting epoch 2/30\n",
      "====================\n",
      "Training Loss: 0.2441, Accuracy: 0.9059\n",
      "Validation Loss: 0.1877, Accuracy: 0.9393\n",
      "====================\n",
      "Starting epoch 3/30\n",
      "====================\n",
      "Training Loss: 0.1960, Accuracy: 0.9233\n",
      "Validation Loss: 0.2291, Accuracy: 0.9281\n",
      "====================\n",
      "Starting epoch 4/30\n",
      "====================\n",
      "Training Loss: 0.1687, Accuracy: 0.9364\n",
      "Validation Loss: 0.2242, Accuracy: 0.9387\n",
      "====================\n",
      "Starting epoch 5/30\n",
      "====================\n",
      "Training Loss: 0.1589, Accuracy: 0.9406\n",
      "Validation Loss: 0.2270, Accuracy: 0.9440\n",
      "====================\n",
      "Starting epoch 6/30\n",
      "====================\n",
      "Training Loss: 0.1227, Accuracy: 0.9528\n",
      "Validation Loss: 0.1862, Accuracy: 0.9446\n",
      "====================\n",
      "Starting epoch 7/30\n",
      "====================\n",
      "Training Loss: 0.1116, Accuracy: 0.9583\n",
      "Validation Loss: 0.1840, Accuracy: 0.9511\n",
      "====================\n",
      "Starting epoch 8/30\n",
      "====================\n",
      "Training Loss: 0.1075, Accuracy: 0.9581\n",
      "Validation Loss: 0.1889, Accuracy: 0.9464\n",
      "====================\n",
      "Starting epoch 9/30\n",
      "====================\n",
      "Training Loss: 0.1061, Accuracy: 0.9591\n",
      "Validation Loss: 0.1784, Accuracy: 0.9532\n",
      "====================\n",
      "Starting epoch 10/30\n",
      "====================\n",
      "Training Loss: 0.1031, Accuracy: 0.9598\n",
      "Validation Loss: 0.2064, Accuracy: 0.9484\n",
      "====================\n",
      "Starting epoch 11/30\n",
      "====================\n",
      "Training Loss: 0.0995, Accuracy: 0.9627\n",
      "Validation Loss: 0.2103, Accuracy: 0.9449\n",
      "====================\n",
      "Starting epoch 12/30\n",
      "====================\n",
      "Training Loss: 0.0987, Accuracy: 0.9623\n",
      "Validation Loss: 0.2117, Accuracy: 0.9458\n",
      "====================\n",
      "Starting epoch 13/30\n",
      "====================\n",
      "Training Loss: 0.1000, Accuracy: 0.9630\n",
      "Validation Loss: 0.2046, Accuracy: 0.9478\n",
      "====================\n",
      "Starting epoch 14/30\n",
      "====================\n",
      "Training Loss: 0.0979, Accuracy: 0.9619\n",
      "Validation Loss: 0.2033, Accuracy: 0.9473\n",
      "====================\n",
      "Starting epoch 15/30\n",
      "====================\n",
      "Training Loss: 0.1008, Accuracy: 0.9612\n",
      "Validation Loss: 0.1944, Accuracy: 0.9446\n",
      "====================\n",
      "Starting epoch 16/30\n",
      "====================\n",
      "Training Loss: 0.0998, Accuracy: 0.9618\n",
      "Validation Loss: 0.2070, Accuracy: 0.9490\n",
      "====================\n",
      "Starting epoch 17/30\n",
      "====================\n",
      "Training Loss: 0.1000, Accuracy: 0.9632\n",
      "Validation Loss: 0.2124, Accuracy: 0.9414\n",
      "====================\n",
      "Starting epoch 18/30\n",
      "====================\n",
      "Training Loss: 0.0988, Accuracy: 0.9629\n",
      "Validation Loss: 0.1998, Accuracy: 0.9478\n",
      "====================\n",
      "Starting epoch 19/30\n",
      "====================\n",
      "Training Loss: 0.1003, Accuracy: 0.9622\n"
     ]
    }
   ],
   "source": [
    "#1 0.8_oversampling_True\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.8,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'oversampling',       #gan, oversampling, None\n",
    "'geoaugment': True,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 12e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.01,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#1 0.2_oversampling_True\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.2,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'oversampling',       #gan, oversampling, None\n",
    "'geoaugment': True,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 12e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.01,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#1 0.2_oversampling_True\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.2,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'oversampling',       #gan, oversampling, None\n",
    "'geoaugment': True,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 16e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.01,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 0.8_gan\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.8,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 32,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#2\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.8,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "            data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#3\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.8,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adamW\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#4 0.6_gan\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.6,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 32,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#5\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.6,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#6\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.6,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 12e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#########################\n",
    "#8 0.4_gan\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.4,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 32,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#9\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.4,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#10\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.4,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 12e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "################################\n",
    "\n",
    "#12 0.2_gan\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.2,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 32,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#13\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.2,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 9e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n",
    "\n",
    "#11\n",
    "params = {\n",
    "'split' : 'orig',\n",
    "'data_ratio' : 0.2,   #1, 0.8, 0.6, 0.4, 0.2\n",
    "'augment' : 'gan',       #gan, oversampling, None\n",
    "'geoaugment': False,\n",
    "'network_name' : \"efficient\", #resnet, vgg, efficient\n",
    "'id': time.strftime(\"%H-%M-%S\"),\n",
    "'loss_function' : \"CrossEntropyLoss\",\n",
    "'optimizer' : \"adam\",\n",
    "'batch_size' : 16,\n",
    "'epochs' : 30, #30/40\n",
    "'adam_lr' : 12e-5,\n",
    "'adam_weight_decay' :0.1,\n",
    "'weight_decay':0.05,\n",
    "'degree':3\n",
    "}\n",
    "\n",
    "network = get_model(params['network_name'])\n",
    "if params['optimizer']==\"adam\":\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=params['adam_lr'])\n",
    "elif params['optimizer']==\"adamW\":\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=params['adam_lr'], weight_decay=params.get('adam_weight_decay',0.1))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset, val_dataset, _ = DatasetMaker(split = params['split'],mode = params['augment'], \n",
    "                                data_ratio = params['data_ratio'],geoaugment = params['geoaugment'],random_affine = params['degree'])\n",
    "\n",
    "neptune_run = neptune.init_run() \n",
    "run_id = neptune_run[\"sys/id\"].fetch()\n",
    "neptune_run['algorithm'] = \"Classifier\"\n",
    "neptune_run['params'] = params\n",
    "net = train(params['epochs'], network, loss_fn, optimizer, params['weight_decay'],\n",
    "                train_dataset, val_dataset, params['batch_size'],shuffle = True, neptune_run = neptune_run)\n",
    "neptune_run.stop()\n",
    "\n",
    "case_string = f\"{params['network_name']}_{params['split']}_{params['data_ratio']}_{params['augment']}_{params['geoaugment']}_{run_id}\"\n",
    "save_history(net, params['network_name'], case_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcb96da45b2fc45e59405d102e32af3d42527a73937a6435f97ad1b01889c6a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
